{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic Regression Basic \n",
    "\n",
    "$$\\hat{y}= {h_\\theta(x)} = \\frac{1}{1+e^{-wx+b}}$$\n",
    "the above equation is the logistic regression model, where w is the weight, b is the bias, and x is the input. \n",
    "The logistic regression model is a linear model with a sigmoid activation function.\n",
    "\n",
    "$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "# Error Function (Cross Entropy Loss)\n",
    "$$J_(w,b_) = {j(\\theta)} = \\frac{1}{N} \\sum_{i=1}^{n}[y^ilog(h_\\theta(x^i))+(1-y^i)log(1-h_\\theta(x^i))]$$\n",
    "\n",
    "# Gradient Descent \n",
    "$$w = w - \\alpha \\frac{\\partial J_(w,b_)}{\\partial w}$$\n",
    "$$b = b - \\alpha \\frac{\\partial J_(w,b_)}{\\partial b}$$\n",
    "\n",
    "# Derivative of the Error Function\n",
    "$$\\frac{\\partial J_(w,b_)}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^{n} (h_\\theta(x^i)-y^i)x^i$$\n",
    "\n",
    "$$\\frac{\\partial J_(w,b_)}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{n} (h_\\theta(x^i)-y^i)$$\n",
    "\n",
    "# Gradient Descent Update Rule\n",
    "$$w = w - \\alpha \\frac{1}{N} \\sum_{i=1}^{n} (h_\\theta(x^i)-y^i)x^i$$\n",
    "$$b = b - \\alpha \\frac{1}{N} \\sum_{i=1}^{n} (h_\\theta(x^i)-y^i)$$\n",
    "\n",
    "# sigmoid graph \n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
    "\n",
    "## Algorithm \n",
    "1. Initialize the weight and bias with random values.\n",
    "2. Calculate the predicted value using the logistic regression model.\n",
    "3. Calculate the error using the cross-entropy loss function.\n",
    "4. Calculate the gradient of the error function.\n",
    "5. Update the weight and bias using the gradient descent update rule.\n",
    "6. Repeat steps 2-5 until the error is minimized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self,lr=0.01,n_iters=1000) -> None:\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None \n",
    "\n",
    "    def _sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        n_samples,n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_model = np.dot(X,self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(linear_model)\n",
    "\n",
    "            dw = (1/n_samples) * np.dot(X.T,(y_pred-y))\n",
    "            db = (1/n_samples) * np.sum(y_pred-y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "        \n",
    "    def predict(self,X):\n",
    "        linear_model = np.dot(X,self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(linear_model)\n",
    "        y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "        return y_pred_cls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
